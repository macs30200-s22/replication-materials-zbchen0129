{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202111ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ml_clf.py\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pylab as plt \n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "import re \n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from contractions import contractions_dict\n",
    "\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score,recall_score,precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test_with_solutions.csv')\n",
    "\n",
    "train_comment = train['Comment']\n",
    "test_comment = test['Comment']\n",
    "train_label = train['Insult']\n",
    "test_label = test['Insult']\n",
    "\n",
    "Data_to_clean = pd.concat([train_comment,test_comment],axis=0)\n",
    "\n",
    "def remove_characters_before_tokenization(text):\n",
    "    text = text.strip()\n",
    "    return re.sub(r'[^a-zA-Z0-9\\' ]', r'', text)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def expand_contra(sentence, contractions_dict):\n",
    "    contras = re.findall(r'\\w+\\'\\w+', sentence)\n",
    "    for i in contras:\n",
    "        expanded_contraction = contractions_dict.get(i)\\\n",
    "                               if contractions_dict.get(i)\\\n",
    "                               else contractions_dict.get(i.lower())\n",
    "        if expanded_contraction:\n",
    "            sentence = re.sub(i, expanded_contraction, sentence)\n",
    "    return sentence\n",
    "\n",
    "def normalize_corpus(corpus, tokenize=False):\n",
    "    normalized_corpus = []\n",
    "    for index, text in enumerate(corpus):\n",
    "        try:\n",
    "            text = expand_contra(text, contractions_dict)\n",
    "        except:\n",
    "            print(index)\n",
    "        text = remove_stopwords(text)\n",
    "        normalized_corpus.append(text)\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "    return normalized_corpus\n",
    "\n",
    "def feat_extract(data,ngram_range):\n",
    "    vectorizer = CountVectorizer(min_df=1,ngram_range=ngram_range)\n",
    "    feature = vectorizer.fit_transform(data)\n",
    "    return(vectorizer,feature)\n",
    "\n",
    "def tfidf_transformer(matrix):\n",
    "    transform = TfidfTransformer(norm='l2',smooth_idf=True,use_idf=True)\n",
    "    tfidf_matrix = transform.fit_transform(matrix)\n",
    "    \n",
    "    return(transform, tfidf_matrix)\n",
    "\n",
    "def metrics(clf_lst, X_test, y_test):\n",
    "    \n",
    "    metrics = []\n",
    "    for clf in clf_lst:\n",
    "        metrics_lst = []\n",
    "        cross_val_score(estimator=clf,X=X_test,y=y_test,cv=5)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        metrics_lst.append(accuracy_score(y_true=y_test,y_pred=y_pred))\n",
    "        metrics_lst.append(f1_score(y_true=y_test,y_pred=y_pred,average='weighted'))\n",
    "        metrics_lst.append(recall_score(y_true=y_test,y_pred=y_pred,average='weighted'))\n",
    "        metrics_lst.append(precision_score(y_true=y_test,y_pred=y_pred,average='weighted'))\n",
    "        \n",
    "        metrics.append(metrics_lst)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Data_to_clean1 = [remove_characters_before_tokenization(i) for i in Data_to_clean]\n",
    "    normalized_data = normalize_corpus(corpus=Data_to_clean1,tokenize=False)\n",
    "    \n",
    "    train_corpus = normalized_data[:3947]\n",
    "    test_corpus = normalized_data[3947:]\n",
    "    \n",
    "    train_vec,train_feat = feat_extract(data=train_corpus,ngram_range=(1,3))\n",
    "    train_features = train_feat.todense()\n",
    "    test_features = train_vec.transform(test_corpus).todense()\n",
    "    \n",
    "    train_transform , train_matrix = tfidf_transformer(train_features)\n",
    "    train_final_feature = train_matrix.todense()\n",
    "    test_final_feature = train_transform.transform(test_features).todense()\n",
    "    \n",
    "    X_training,X_testing=sparse.csr_matrix(train_final_feature),sparse.csr_matrix(test_final_feature)\n",
    "    \n",
    "    classifiers_lst = []\n",
    "    \n",
    "    NB = MultinomialNB()\n",
    "    NB.fit(X=X_training,y=train_label)\n",
    "    classifiers_lst.append(NB)\n",
    "    \n",
    "    SGD = SGDClassifier()\n",
    "    SGD.fit(X=X_training,y=train_label)\n",
    "    classifiers_lst.append(SGD)\n",
    "    \n",
    "    LogReg = LogisticRegression()\n",
    "    LogReg.fit(X=X_training,y=train_label)\n",
    "    classifiers_lst.append(LogReg)\n",
    "    \n",
    "    GB = GradientBoostingClassifier()\n",
    "    GB.fit(X=X_training,y=train_label)\n",
    "    classifiers_lst.append(GB)\n",
    "    \n",
    "    RF = RandomForestClassifier()\n",
    "    RF.fit(X=X_training,y=train_label)\n",
    "    classifiers_lst.append(RF)\n",
    "    \n",
    "    metrics = metrics(classifiers_lst, X_testing, test_label)  \n",
    "    \n",
    "    df = pd.DataFrame(metrics, \n",
    "             columns=['Accuracy', 'F1_score', 'recall_score', 'precision_score'], \n",
    "             index = ['Naive Bayes', \"SGD\", \"Logistic Regression\", 'GradientBoosting', 'RandomForest'])\n",
    "    \n",
    "    df.T.plot()\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dcb930b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pylab as plt \n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "import re \n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from contractions import contractions_dict\n",
    "\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "def remove_characters_before_tokenization(text):\n",
    "    text = text.strip()\n",
    "    return re.sub(r'[^a-zA-Z0-9\\' ]', r'', text)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def expand_contra(sentence, contractions_dict):\n",
    "    contras = re.findall(r'\\w+\\'\\w+', sentence)\n",
    "    for i in contras:\n",
    "        expanded_contraction = contractions_dict.get(i)\\\n",
    "                               if contractions_dict.get(i)\\\n",
    "                               else contractions_dict.get(i.lower())\n",
    "        if expanded_contraction:\n",
    "            sentence = re.sub(i, expanded_contraction, sentence)\n",
    "    return sentence\n",
    "\n",
    "def normalize_corpus(corpus, tokenize=False):\n",
    "    normalized_corpus = []\n",
    "    for index, text in enumerate(corpus):\n",
    "        try:\n",
    "            text = expand_contra(text, contractions_dict)\n",
    "        except:\n",
    "            print(index)\n",
    "        text = remove_stopwords(text)\n",
    "        normalized_corpus.append(text)\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "    return normalized_corpus\n",
    "\n",
    "def feat_extract(data,ngram_range):\n",
    "    vectorizer = CountVectorizer(min_df=1,ngram_range=ngram_range)\n",
    "    feature = vectorizer.fit_transform(data)\n",
    "    return(vectorizer,feature)\n",
    "\n",
    "def tfidf_transformer(matrix):\n",
    "    transform = TfidfTransformer(norm='l2',smooth_idf=True,use_idf=True)\n",
    "    tfidf_matrix = transform.fit_transform(matrix)\n",
    "    \n",
    "    return(transform, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d78f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ml_clf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ml_clf.py\n",
    "import utils\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score,recall_score,precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test_with_solutions.csv')\n",
    "\n",
    "train_comment = train['Comment']\n",
    "test_comment = test['Comment']\n",
    "train_label = train['Insult']\n",
    "test_label = test['Insult']\n",
    "\n",
    "\n",
    "\n",
    "def data_clean(train_comment, test_comment):\n",
    "    Data_to_clean = pd.concat([train_comment,test_comment],axis=0)\n",
    "    Data_to_clean1 = [utils.remove_characters_before_tokenization(i) for i in Data_to_clean]\n",
    "    normalized_data = utils.normalize_corpus(corpus=Data_to_clean1,tokenize=False)\n",
    "    \n",
    "    train_corpus = normalized_data[:3947]\n",
    "    test_corpus = normalized_data[3947:]\n",
    "    \n",
    "    train_vec,train_feat = utils.feat_extract(data=train_corpus,ngram_range=(1,3))\n",
    "    train_features = train_feat.todense()\n",
    "    test_features = train_vec.transform(test_corpus).todense()\n",
    "    \n",
    "    train_transform , train_matrix = utils.tfidf_transformer(train_features)\n",
    "    train_final_feature = train_matrix.todense()\n",
    "    test_final_feature = train_transform.transform(test_features).todense()\n",
    "    \n",
    "    X_training,X_testing=sparse.csr_matrix(train_final_feature),sparse.csr_matrix(test_final_feature)\n",
    "    \n",
    "    return X_training, X_testing\n",
    "\n",
    "def ml_implement(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    classifiers_lst = []\n",
    "    \n",
    "    NB = MultinomialNB()\n",
    "    NB.fit(X=X_train,y=y_train)\n",
    "    classifiers_lst.append(NB)\n",
    "    \n",
    "    SGD = SGDClassifier()\n",
    "    SGD.fit(X=X_train,y=y_train)\n",
    "    classifiers_lst.append(SGD)\n",
    "    \n",
    "    LogReg = LogisticRegression()\n",
    "    LogReg.fit(X=X_train,y=y_train)\n",
    "    classifiers_lst.append(LogReg)\n",
    "    \n",
    "    GB = GradientBoostingClassifier()\n",
    "    GB.fit(X=X_train,y=y_train)\n",
    "    classifiers_lst.append(GB)\n",
    "    \n",
    "    RF = RandomForestClassifier()\n",
    "    RF.fit(X=X_train,y=y_train)\n",
    "    classifiers_lst.append(RF)\n",
    "    \n",
    "    metrics = utils.metrics(classifiers_lst, X_test, y_test)  \n",
    "    \n",
    "    df = pd.DataFrame(metrics, \n",
    "             columns=['Accuracy', 'F1_score', 'recall_score', 'precision_score'], \n",
    "             index = ['Naive Bayes', \"SGD\", \"Logistic Regression\", 'GradientBoosting', 'RandomForest'])\n",
    "    \n",
    "    df.T.plot(kind='bar', figsize = (10, 10), )\n",
    "    plt.xticks(rotation=360)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig('clf_metrics.png')\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    X_train, X_test = data_clean(train_comment, test_comment)\n",
    "    clf_df = ml_implement(X_train, train_label, X_test, test_label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd541355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  FutureWarning,\n",
      "/Library/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  FutureWarning,\n",
      "/Library/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cross_val_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/uchicago/Computational research/replication-materials-zbchen0129/ml_clf.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_comment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_comment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mclf_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mml_implement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/uchicago/Computational research/replication-materials-zbchen0129/ml_clf.py\u001b[0m in \u001b[0;36mml_implement\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mclassifiers_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     df = pd.DataFrame(metrics, \n",
      "\u001b[0;32m~/Desktop/uchicago/Computational research/replication-materials-zbchen0129/utils.py\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(clf_lst, X_test, y_test)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclf_lst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mmetrics_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cross_val_score' is not defined"
     ]
    }
   ],
   "source": [
    "run ml_clf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a146c7e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (ml_clf.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/Users/chenzhibin/Desktop/uchicago/Computational research/replication-materials-zbchen0129/ml_clf.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    %load_ext autoreload\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "run ml_clf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acb84719",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "111c5629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  FutureWarning,\n",
      "/Library/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  FutureWarning,\n",
      "/Library/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cross_val_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/uchicago/Computational research/replication-materials-zbchen0129/ml_clf.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_comment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_comment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mclf_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mml_implement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/uchicago/Computational research/replication-materials-zbchen0129/ml_clf.py\u001b[0m in \u001b[0;36mml_implement\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mclassifiers_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     df = pd.DataFrame(metrics, \n",
      "\u001b[0;32m~/Desktop/uchicago/Computational research/replication-materials-zbchen0129/utils.py\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(clf_lst, X_test, y_test)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclf_lst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mmetrics_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cross_val_score' is not defined"
     ]
    }
   ],
   "source": [
    "run ml_clf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34caf356",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nz/jn46yvw155q0l_6bpm5shgdc0000gn/T/ipykernel_5279/1736104788.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manti_vac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'antiva_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "anti_vac = pd.read_csv('antiva_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73442632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>date</th>\n",
       "      <th>timezone</th>\n",
       "      <th>place</th>\n",
       "      <th>tweet</th>\n",
       "      <th>language</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>...</th>\n",
       "      <th>source</th>\n",
       "      <th>user_rt_id</th>\n",
       "      <th>user_rt</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>retweet_date</th>\n",
       "      <th>translate</th>\n",
       "      <th>trans_src</th>\n",
       "      <th>trans_dest</th>\n",
       "      <th>text_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1514630855423332354</td>\n",
       "      <td>1514630855423332354</td>\n",
       "      <td>1.649951e+12</td>\n",
       "      <td>2022-04-14 10:45:11</td>\n",
       "      <td>-500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Here’s another milestone unlocked! We are deli...</td>\n",
       "      <td>en</td>\n",
       "      <td>['covlex']</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1514630825174003714</td>\n",
       "      <td>1514630825174003714</td>\n",
       "      <td>1.649951e+12</td>\n",
       "      <td>2022-04-14 10:45:03</td>\n",
       "      <td>-500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>India Covid-19 Vaccination Update: 14-Apr-2022...</td>\n",
       "      <td>en</td>\n",
       "      <td>['largestvaccinedrive', 'largestvaccinationdri...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1514630801085775875</td>\n",
       "      <td>1514615791005577229</td>\n",
       "      <td>1.649951e+12</td>\n",
       "      <td>2022-04-14 10:44:58</td>\n",
       "      <td>-500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@CTVNews Healthy kids are better off without v...</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'CTVNews', 'name': 'CTV News'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1514630751521812480</td>\n",
       "      <td>1514418518946582531</td>\n",
       "      <td>1.649951e+12</td>\n",
       "      <td>2022-04-14 10:44:46</td>\n",
       "      <td>-500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@ZaleskiLuke @Acyn And the rest of the Greed O...</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'ZaleskiLuke', 'name': 'Luke ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1514630611209789442</td>\n",
       "      <td>1514630611209789442</td>\n",
       "      <td>1.649951e+12</td>\n",
       "      <td>2022-04-14 10:44:12</td>\n",
       "      <td>-500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Mobile Vaccination Unit will be back at Mo...</td>\n",
       "      <td>en</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   id      conversation_id    created_at  \\\n",
       "0           0  1514630855423332354  1514630855423332354  1.649951e+12   \n",
       "1           1  1514630825174003714  1514630825174003714  1.649951e+12   \n",
       "2           3  1514630801085775875  1514615791005577229  1.649951e+12   \n",
       "3           6  1514630751521812480  1514418518946582531  1.649951e+12   \n",
       "4          10  1514630611209789442  1514630611209789442  1.649951e+12   \n",
       "\n",
       "                  date  timezone place  \\\n",
       "0  2022-04-14 10:45:11      -500   NaN   \n",
       "1  2022-04-14 10:45:03      -500   NaN   \n",
       "2  2022-04-14 10:44:58      -500   NaN   \n",
       "3  2022-04-14 10:44:46      -500   NaN   \n",
       "4  2022-04-14 10:44:12      -500   NaN   \n",
       "\n",
       "                                               tweet language  \\\n",
       "0  Here’s another milestone unlocked! We are deli...       en   \n",
       "1  India Covid-19 Vaccination Update: 14-Apr-2022...       en   \n",
       "2  @CTVNews Healthy kids are better off without v...       en   \n",
       "3  @ZaleskiLuke @Acyn And the rest of the Greed O...       en   \n",
       "4  Our Mobile Vaccination Unit will be back at Mo...       en   \n",
       "\n",
       "                                            hashtags  ... source  user_rt_id  \\\n",
       "0                                         ['covlex']  ...    NaN         NaN   \n",
       "1  ['largestvaccinedrive', 'largestvaccinationdri...  ...    NaN         NaN   \n",
       "2                                                 []  ...    NaN         NaN   \n",
       "3                                                 []  ...    NaN         NaN   \n",
       "4                                                 []  ...    NaN         NaN   \n",
       "\n",
       "   user_rt retweet_id                                           reply_to  \\\n",
       "0      NaN        NaN                                                 []   \n",
       "1      NaN        NaN                                                 []   \n",
       "2      NaN        NaN  [{'screen_name': 'CTVNews', 'name': 'CTV News'...   \n",
       "3      NaN        NaN  [{'screen_name': 'ZaleskiLuke', 'name': 'Luke ...   \n",
       "4      NaN        NaN                                                 []   \n",
       "\n",
       "   retweet_date  translate trans_src trans_dest text_tokens  \n",
       "0           NaN        NaN       NaN        NaN         NaN  \n",
       "1           NaN        NaN       NaN        NaN         NaN  \n",
       "2           NaN        NaN       NaN        NaN         NaN  \n",
       "3           NaN        NaN       NaN        NaN         NaN  \n",
       "4           NaN        NaN       NaN        NaN         NaN  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anti_vac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18fded6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9432 entries, 0 to 9431\n",
      "Data columns (total 40 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Unnamed: 0       9432 non-null   int64  \n",
      " 1   id               9432 non-null   int64  \n",
      " 2   conversation_id  9432 non-null   int64  \n",
      " 3   created_at       9432 non-null   float64\n",
      " 4   date             9432 non-null   object \n",
      " 5   timezone         9432 non-null   int64  \n",
      " 6   place            1 non-null      object \n",
      " 7   tweet            9432 non-null   object \n",
      " 8   language         9432 non-null   object \n",
      " 9   hashtags         9432 non-null   object \n",
      " 10  cashtags         9432 non-null   object \n",
      " 11  user_id          9432 non-null   int64  \n",
      " 12  user_id_str      9432 non-null   int64  \n",
      " 13  username         9432 non-null   object \n",
      " 14  name             9430 non-null   object \n",
      " 15  day              9432 non-null   int64  \n",
      " 16  hour             9432 non-null   int64  \n",
      " 17  link             9432 non-null   object \n",
      " 18  urls             9432 non-null   object \n",
      " 19  photos           9432 non-null   object \n",
      " 20  video            9432 non-null   int64  \n",
      " 21  thumbnail        1301 non-null   object \n",
      " 22  retweet          9432 non-null   bool   \n",
      " 23  nlikes           9432 non-null   int64  \n",
      " 24  nreplies         9432 non-null   int64  \n",
      " 25  nretweets        9432 non-null   int64  \n",
      " 26  quote_url        834 non-null    object \n",
      " 27  search           9432 non-null   object \n",
      " 28  near             0 non-null      float64\n",
      " 29  geo              0 non-null      float64\n",
      " 30  source           0 non-null      float64\n",
      " 31  user_rt_id       0 non-null      float64\n",
      " 32  user_rt          0 non-null      float64\n",
      " 33  retweet_id       0 non-null      float64\n",
      " 34  reply_to         9432 non-null   object \n",
      " 35  retweet_date     0 non-null      float64\n",
      " 36  translate        0 non-null      float64\n",
      " 37  trans_src        0 non-null      float64\n",
      " 38  trans_dest       0 non-null      float64\n",
      " 39  text_tokens      8398 non-null   object \n",
      "dtypes: bool(1), float64(11), int64(12), object(16)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "anti_vac.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c77eb610",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = anti_vac['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b54a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a01e033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7100ee37",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'da' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nz/jn46yvw155q0l_6bpm5shgdc0000gn/T/ipykernel_5279/3099675764.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mData_to_clean1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_characters_before_tokenization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mda\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnormalized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mData_to_clean1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'da' is not defined"
     ]
    }
   ],
   "source": [
    "Data_to_clean1 = [utils.remove_characters_before_tokenization(i) for i in da]\n",
    "normalized_data = utils.normalize_corpus(corpus=Data_to_clean1,tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14c95909",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nz/jn46yvw155q0l_6bpm5shgdc0000gn/T/ipykernel_5279/1839279845.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalized_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_transform\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfidf_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_final_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "train_vec,train_feat = utils.feat_extract(data=normalized_data,ngram_range=(1,3))\n",
    "train_features = train_feat.todense()\n",
    "\n",
    "train_transform , train_matrix = utils.tfidf_transformer(train_features)\n",
    "train_final_feature = train_matrix.todense()\n",
    "\n",
    "X_training = sparse.csr_matrix(train_final_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c35ba8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pc_analysis.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pc_analysis.py\n",
    "import utils\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def data_clean(df):\n",
    "    data = [utils.remove_characters_before_tokenization(i) for i in df]\n",
    "    data1= utils.normalize_corpus(corpus=data,tokenize=False)\n",
    "    \n",
    "    text = \" \".join(tweet for tweet in data1)\n",
    "    return text\n",
    "\n",
    "def plot_wordcloud(text):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update(list(string.punctuation) + [\"https\", \"people\", 'think', 'will', 's', 'others', \"one\", \"politically correct\", \"politically\", \"correct\", \"political correctness\", \"political\", \"correctness\", \"sensitive\", 'covid','covid-19', 'covid19', \"vaccines\", 'vaxxer', 'vaxxers', 't', 'co', 'pandemic', 'anti-vaccine', 'amp'] + ['considerate', 'diplomatic', 'gender free', 'inclusive', 'inoffensive', 'multicultural', 'multiculturally sensitive', 'politic', 'respectful', 'sensitive', 'sensitive to others', 'bias free', 'liberal', 'nondiscriminatory', 'nonracist', 'nonsexist', 'unbiased', 'political correctness', 'politically correct'])\n",
    "\n",
    "\n",
    "\n",
    "    wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", min_word_length=4, collocation_threshold=4).generate_from_text(text)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('pc_wordcloud.png')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    df_pc= pd.read_csv('pc_dataset.csv', index_col=0)\n",
    "    text = data_clean(df_pc['tweet'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc9063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
